{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6234848,"sourceType":"datasetVersion","datasetId":3581652},{"sourceId":6235803,"sourceType":"datasetVersion","datasetId":3582321}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:51:34.009563Z","iopub.execute_input":"2024-12-03T11:51:34.010092Z","iopub.status.idle":"2024-12-03T11:51:45.739727Z","shell.execute_reply.started":"2024-12-03T11:51:34.010048Z","shell.execute_reply":"2024-12-03T11:51:45.738327Z"}},"outputs":[{"name":"stdout","text":"Collecting torchio\n  Using cached torchio-0.20.2-py3-none-any.whl.metadata (50 kB)\nRequirement already satisfied: deprecated in /opt/conda/lib/python3.10/site-packages (from torchio) (1.2.14)\nRequirement already satisfied: humanize in /opt/conda/lib/python3.10/site-packages (from torchio) (4.9.0)\nRequirement already satisfied: nibabel in /opt/conda/lib/python3.10/site-packages (from torchio) (5.2.1)\nRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.10/site-packages (from torchio) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torchio) (1.14.1)\nRequirement already satisfied: simpleitk!=2.0.*,!=2.1.1.1 in /opt/conda/lib/python3.10/site-packages (from torchio) (2.4.0)\nRequirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.10/site-packages (from torchio) (2.4.0+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchio) (4.66.4)\nRequirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from torchio) (0.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->torchio) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->torchio) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->torchio) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->torchio) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->torchio) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->torchio) (2024.6.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated->torchio) (1.16.0)\nRequirement already satisfied: packaging>=17 in /opt/conda/lib/python3.10/site-packages (from nibabel->torchio) (21.3)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer->torchio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->torchio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer->torchio) (13.7.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17->nibabel->torchio) (3.1.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer->torchio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer->torchio) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.1->torchio) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.1->torchio) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->torchio) (0.1.2)\nUsing cached torchio-0.20.2-py3-none-any.whl (175 kB)\nInstalling collected packages: torchio\nSuccessfully installed torchio-0.20.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os, glob, nibabel, random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch import Tensor\nimport torchio as tio\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt","metadata":{"_uuid":"5b4f5ff5-7b84-42ae-8a23-cd46933933ff","_cell_guid":"72b740df-75ba-496d-ad56-8364f8cc3d58","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-03T11:51:45.741925Z","iopub.execute_input":"2024-12-03T11:51:45.742300Z","iopub.status.idle":"2024-12-03T11:51:47.112133Z","shell.execute_reply.started":"2024-12-03T11:51:45.742263Z","shell.execute_reply":"2024-12-03T11:51:47.111011Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def double_conv(in_channel, out_channel):\n    conv = nn.Sequential(\n        nn.Conv2d(in_channel, out_channel, kernel_size=3),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channel, out_channel, kernel_size=3),\n        nn.ReLU(inplace=True)\n    )\n    return conv\n\ndef crop_img(tensor, target_tensor):\n    target_size = target_tensor.size()[2]\n    tensor_size = tensor.size()[2]\n    delta = (tensor_size - target_size) // 2\n    return tensor[:, :, delta:tensor_size - delta, delta:tensor_size - delta]","metadata":{"_uuid":"45f08e6e-07d3-450b-8fc3-f004c658b231","_cell_guid":"5433b376-35db-4e69-a92a-75757bffe786","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-03T11:51:47.113443Z","iopub.execute_input":"2024-12-03T11:51:47.113989Z","iopub.status.idle":"2024-12-03T11:51:47.121212Z","shell.execute_reply.started":"2024-12-03T11:51:47.113954Z","shell.execute_reply":"2024-12-03T11:51:47.120051Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# checking devices available\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:51:47.123736Z","iopub.execute_input":"2024-12-03T11:51:47.124995Z","iopub.status.idle":"2024-12-03T11:51:47.139617Z","shell.execute_reply.started":"2024-12-03T11:51:47.124935Z","shell.execute_reply":"2024-12-03T11:51:47.138470Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Data path\nroot_path = '/kaggle/input/brats2023-ssa-training-dataset/ASNR-MICCAI-BraTS2023-SSA-Challenge-TrainingData_V2/BraTS-SSA-00007-000/'","metadata":{"_uuid":"8b9756b6-153f-4ac1-8218-e2565aa5c625","_cell_guid":"2ae119d7-b651-4b1c-b610-7d7573c100a5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-03T11:51:47.141075Z","iopub.execute_input":"2024-12-03T11:51:47.141435Z","iopub.status.idle":"2024-12-03T11:51:47.157261Z","shell.execute_reply.started":"2024-12-03T11:51:47.141403Z","shell.execute_reply":"2024-12-03T11:51:47.155015Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\nclass Train_dataset(Dataset):\n    def __init__(self, img_path, transform):\n        self.img_path = img_path\n        self.transform = transform \n\n    def __len__(self):\n        return len(self.img_path)\n\n    def __getitem__(self, index):\n        img_path = glob.glob(self.img_path)[index]\n        \n        mask_path = glob.glob(os.path.join(img_path, '**', '*seg.nii'), recursive=True)[0]\n        t1_path = glob.glob(os.path.join(img_path, '**', '*t1n.nii'), recursive=True)[0]\n        t2_path = glob.glob(os.path.join(img_path, '**', '*t2w.nii'), recursive=True)[0]\n        print(mask_path)\n        t1 = nibabel.load(t1_path)\n        t1_data = t1.get_fdata()\n        \n        t2 = nibabel.load(t2_path)\n        t2_data = t2.get_fdata()\n\n        mask_img = nibabel.load(mask_path)\n        mask_data = t1.get_fdata()\n\n        if self.transform:\n            t1_data = self.transform(t1_data)\n            t2_data = self.transform(1, t2_data)\n            mask_data = self.transform(1, mask_data)\n            \n        \n        image = np.stack([t1_data, t2_data], axis=0)\n\n        return torch.tensor(image, dtype=torch.float32), torch.tensor(mask_data, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T12:37:18.574006Z","iopub.execute_input":"2024-12-03T12:37:18.574402Z","iopub.status.idle":"2024-12-03T12:37:18.583575Z","shell.execute_reply.started":"2024-12-03T12:37:18.574367Z","shell.execute_reply":"2024-12-03T12:37:18.582219Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"transform = tio.Compose([\n    tio.RandomAffine(),\n    tio.RandomElasticDeformation(),\n    tio.RescaleIntensity((0, 1))\n])\n\ndataset = Train_dataset(root_path, transform)\ndataloader = DataLoader(dataset, batch_size=10, num_workers=2, shuffle=True)\n\nrandom_img, random_mask = dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T12:37:18.739045Z","iopub.execute_input":"2024-12-03T12:37:18.739573Z","iopub.status.idle":"2024-12-03T12:37:18.937258Z","shell.execute_reply.started":"2024-12-03T12:37:18.739527Z","shell.execute_reply":"2024-12-03T12:37:18.935670Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/brats2023-ssa-training-dataset/ASNR-MICCAI-BraTS2023-SSA-Challenge-TrainingData_V2/BraTS-SSA-00007-000/BraTS-SSA-00007-000-seg.nii\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[73], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Train_dataset(root_path, transform)\n\u001b[1;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m random_img, random_mask \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n","Cell \u001b[0;32mIn[72], line 26\u001b[0m, in \u001b[0;36mTrain_dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 26\u001b[0m     t1_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     t2_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;241m1\u001b[39m, t2_data)\n\u001b[1;32m     28\u001b[0m     mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;241m1\u001b[39m, mask_data)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchio/transforms/transform.py:162\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    160\u001b[0m     subject \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(subject)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m, under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 162\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, image \u001b[38;5;129;01min\u001b[39;00m images_to_keep\u001b[38;5;241m.\u001b[39mitems():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchio/transforms/augmentation/composition.py:49\u001b[0m, in \u001b[0;36mCompose.apply_transform\u001b[0;34m(self, subject)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, subject: Subject) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Subject:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 49\u001b[0m         subject \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m subject\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchio/transforms/transform.py:151\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_input:\n\u001b[1;32m    146\u001b[0m     data_parser \u001b[38;5;241m=\u001b[39m DataParser(\n\u001b[1;32m    147\u001b[0m         data,\n\u001b[1;32m    148\u001b[0m         keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude,\n\u001b[1;32m    149\u001b[0m         label_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_keys,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m     subject \u001b[38;5;241m=\u001b[39m \u001b[43mdata_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_subject\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     subject \u001b[38;5;241m=\u001b[39m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchio/transforms/data_parser.py:60\u001b[0m, in \u001b[0;36mDataParser.get_subject\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_nib \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, (np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[0;32m---> 60\u001b[0m     subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchio/transforms/data_parser.py:120\u001b[0m, in \u001b[0;36mDataParser._parse_tensor\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    114\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe input must be a 4D tensor with dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (channels, x, y, z) but it has shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Tips: if it is a volume, please add the channels dimension;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if it is 2D, also add a dimension of size 1 for the z axis\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_subject_from_tensor(data)\n","\u001b[0;31mValueError\u001b[0m: The input must be a 4D tensor with dimensions (channels, x, y, z) but it has shape (240, 240, 155). Tips: if it is a volume, please add the channels dimension; if it is 2D, also add a dimension of size 1 for the z axis"],"ename":"ValueError","evalue":"The input must be a 4D tensor with dimensions (channels, x, y, z) but it has shape (240, 240, 155). Tips: if it is a volume, please add the channels dimension; if it is 2D, also add a dimension of size 1 for the z axis","output_type":"error"}],"execution_count":73},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.max_pool_2x2 = nn.MaxPool2d(stride=2, kernel_size=2)\n        self.down_conv_1 = double_conv(1, 64)\n        self.down_conv_2 = double_conv(64, 128)\n        self.down_conv_3 = double_conv(128, 256)\n        self.down_conv_4 = double_conv(256, 512)\n        self.down_conv_5 = double_conv(512, 1024)\n\n        self.up_trans_1 = nn.ConvTranspose2d(\n            in_channels = 1024,\n            out_channels = 512,\n            stride = 2,\n            kernel_size = 2\n        )\n        self.up_conv_1 = double_conv(1024, 512)\n\n        self.up_trans_2 = nn.ConvTranspose2d(\n            in_channels = 512,\n            out_channels = 256,\n            stride = 2,\n            kernel_size = 2\n        )\n        self.up_conv_2 = double_conv(512, 256)\n\n        self.up_trans_3 = nn.ConvTranspose2d(\n            in_channels = 256,\n            out_channels = 128,\n            stride = 2,\n            kernel_size = 2\n        )\n        self.up_conv_3 = double_conv(256, 128)\n\n        self.up_trans_4 = nn.ConvTranspose2d(\n            in_channels = 128,\n            out_channels = 64,\n            stride = 2,\n            kernel_size = 2\n        )\n        self.up_conv_4 = double_conv(128, 64)\n        \n        self.output = nn.Conv2d(\n            in_channels = 64,\n            out_channels = 2,\n            kernel_size = 1\n        )\n\n    def forward(self, image):\n        #encoder\n        x1 = self.down_conv_1(image)\n        x2 = self.max_pool_2x2(x1)\n        x3 = self.down_conv_2(x2)\n        x4 = self.max_pool_2x2(x3)\n        x5 = self.down_conv_3(x4)\n        x6 = self.max_pool_2x2(x5)\n        x7 = self.down_conv_4(x6)\n        x8 = self.max_pool_2x2(x7)\n        x9 = self.down_conv_5(x8)\n\n        x = self.up_trans_1(x9)\n        y = crop_img(x7, x)\n        x = self.up_conv_1(torch.cat([x, y], 1))\n\n        x = self.up_trans_2(x)\n        y = crop_img(x5, x)\n        x = self.up_conv_2(torch.cat([x, y], 1))\n\n        x = self.up_trans_3(x)\n        y = crop_img(x3, x)\n        x = self.up_conv_3(torch.cat([x, y], 1))\n\n        x = self.up_trans_4(x)\n        y = crop_img(x1, x)\n        x = self.up_conv_4(torch.cat([x, y], 1))\n\n        output = self.output(x)\n        return output\n\n\nif __name__ == '__main__':\n    model = UNet()\n    image = torch.rand([1, 1, 572, 572])\n    model.forward(image)","metadata":{"_uuid":"944d8694-d1aa-45b5-8fa8-a2ab830883b9","_cell_guid":"f9239acb-a66b-43c4-871b-cf4d7151000d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-03T11:51:47.306362Z","iopub.status.idle":"2024-12-03T11:51:47.306765Z","shell.execute_reply.started":"2024-12-03T11:51:47.306588Z","shell.execute_reply":"2024-12-03T11:51:47.306609Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}